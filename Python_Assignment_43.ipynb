{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpKxcSfKVR-V"
      },
      "outputs": [],
      "source": [
        "# Q1\n",
        "\n",
        "\"\"\" Key Features of the Wine Quality Data Set\n",
        "The wine quality data set is a widely used resource in the field of machine learning and data analysis, particularly for predicting the quality of wines based on various\n",
        " physicochemical properties. This dataset typically includes several features that are crucial for determining wine quality. Each feature plays a significant role in influencing\n",
        " the sensory characteristics and overall perception of wine quality. Below, we discuss these key features and their importance:\n",
        "\n",
        "1. Fixed Acidity\n",
        "Fixed acidity refers to non-volatile acids that do not evaporate easily, such as tartaric acid. It is a critical component in maintaining the stability and longevity of wine. High\n",
        " levels of fixed acidity can contribute to a crisp taste, which is often desirable in white wines. However, excessive acidity can lead to an overly sour taste.\n",
        "\n",
        "2. Volatile Acidity\n",
        "Volatile acidity primarily consists of acetic acid, which can impart vinegar-like flavors if present in high concentrations. It is an essential indicator of spoilage or poor\n",
        "fermentation practices. Low levels are generally acceptable and can add complexity to the aroma profile.\n",
        "\n",
        "3. Citric Acid\n",
        "Citric acid adds freshness and enhances flavor complexity in wines. It can also act as a preservative by inhibiting microbial growth. The presence of citric acid is more common\n",
        "in white wines than reds.\n",
        "\n",
        "4. Residual Sugar\n",
        "Residual sugar refers to the amount of sugar remaining after fermentation stops, which affects sweetness levels in wine. While dry wines have low residual sugar content, sweeter\n",
        "varieties will have higher amounts, impacting consumer preference significantly.\n",
        "\n",
        "5. Chlorides\n",
        "Chlorides contribute to the saltiness perception in wine and can influence its overall balance and mouthfeel. Excessive chloride levels may result from environmental factors or\n",
        " winemaking processes and can negatively affect taste.\n",
        "\n",
        "6. Free Sulfur Dioxide\n",
        "Free sulfur dioxide acts as an antioxidant and antimicrobial agent, preserving wine's freshness by preventing oxidation and spoilage from bacteria or yeast growth.\n",
        "\n",
        "7. Total Sulfur Dioxide\n",
        "Total sulfur dioxide includes both free and bound forms; it ensures long-term preservation but must be carefully managed due to potential health concerns for sensitive individuals.\n",
        "\n",
        "8. Density\n",
        "Density correlates with alcohol content; higher densities usually indicate lower alcohol levels since alcohol has less density than water.\n",
        "\n",
        "9. pH Level\n",
        "The pH level measures acidity/alkalinity balance within wine—lower pH values indicate higher acidity—and influences color stability (especially important for red wines) along with\n",
        " microbial stability during storage/aging processes.\n",
        "\n",
        "10. Sulphates\n",
        "Sulphates enhance flavor intensity while acting as preservatives against oxidation/spoilage organisms like bacteria/yeast strains found naturally occurring throughout production\n",
        " stages until bottling occurs finally!\n",
        "\n",
        "11. Alcohol Content\n",
        "Alcohol content significantly impacts body/mouthfeel perceptions alongside aromatic profiles experienced upon tasting different varietals produced globally today!\n",
        "\n",
        "Each feature contributes uniquely towards predicting overall quality scores assigned through sensory evaluations conducted by trained panels assessing attributes such as\n",
        "aroma/flavor intensity/balance/harmony among others considered vital when determining final ratings given ultimately!\n",
        "\n",
        "Importance of Features\n",
        "Understanding these features' roles helps winemakers optimize production methods tailored specifically towards achieving desired outcomes aligned closely alongside consumer\n",
        "preferences evolving constantly over time! By analyzing relationships between physicochemical properties & perceived qualities objectively using statistical models/machine learning\n",
        "algorithms available today researchers gain insights into complex interactions underlying successful vintages created worldwide annually!\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\n",
        "\"\"\" Handling Missing Data in the Wine Quality Dataset During Feature Engineering\n",
        "Handling missing data is a critical step in the feature engineering process, especially when working with datasets like the wine quality dataset. This dataset typically includes\n",
        "various chemical properties of wines, such as acidity, sugar content, pH levels, and alcohol concentration, which are used to predict wine quality. The presence of missing data\n",
        "can significantly affect the performance of predictive models if not addressed properly.\n",
        "\n",
        "Techniques for Handling Missing Data\n",
        "1. Deletion Methods\n",
        "Listwise Deletion\n",
        "Listwise deletion involves removing any row with at least one missing value. This method is straightforward and maintains the integrity of the dataset by ensuring that only\n",
        "complete cases are analyzed. However, it can lead to significant data loss if many observations have missing values.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simplicity and ease of implementation.\n",
        "Maintains consistency across analyses since all analyses use the same set of observations.\n",
        "Disadvantages:\n",
        "\n",
        "Can result in substantial data loss, reducing statistical power.\n",
        "May introduce bias if the missing data is not completely random (MCAR).\n",
        "Pairwise Deletion\n",
        "Pairwise deletion retains more data by using all available information for each analysis. It calculates statistics using all cases where the variables of interest are present.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Retains more data compared to listwise deletion.\n",
        "Useful when different analyses require different subsets of variables.\n",
        "Disadvantages:\n",
        "\n",
        "Can lead to inconsistencies across analyses due to varying sample sizes.\n",
        "More complex to implement and interpret results.\n",
        "2. Imputation Methods\n",
        "Mean/Median/Mode Imputation\n",
        "This technique replaces missing values with the mean (for continuous variables), median, or mode (for categorical variables) of the observed data.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and quick to implement.\n",
        "Preserves sample size by filling in gaps with plausible values.\n",
        "Disadvantages:\n",
        "\n",
        "Reduces variability in the dataset, potentially leading to biased estimates.\n",
        "Assumes that missing values are similar to observed ones, which may not be true.\n",
        "Regression Imputation\n",
        "Regression imputation predicts missing values using a regression model based on other available variables in the dataset. For example, if alcohol content is missing, it might be\n",
        " predicted using other chemical properties like acidity or sugar content.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Utilizes relationships between variables to provide more informed imputations.\n",
        "Can improve model accuracy compared to simpler methods like mean imputation.\n",
        "Disadvantages:\n",
        "\n",
        "Assumes linear relationships between variables which may not exist.\n",
        "Can underestimate variability because predicted values tend toward central tendencies.\n",
        "Multiple Imputation\n",
        "Multiple imputation involves creating several different plausible datasets by imputing missing values multiple times and then combining results from these datasets for analysis.\n",
        "This approach accounts for uncertainty around what those missing values could be.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Provides robust estimates by incorporating variability due to imputation.\n",
        "Reduces bias compared to single imputation methods by considering multiple possibilities for each missing value.\n",
        "Disadvantages:\n",
        "\n",
        "Computationally intensive and complex to implement.\n",
        "Requires careful consideration of model assumptions and parameters used during imputation.\n",
        "3. Advanced Techniques\n",
        "K-nearest Neighbors (KNN) Imputation\n",
        "KNN imputation fills in missing values based on similar instances within the dataset. It identifies 'k' nearest neighbors based on distance metrics like Euclidean distance and\n",
        "uses their average or most frequent value for imputation.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Captures local structure within data better than global methods like mean imputation.\n",
        "Flexible as it does not assume linear relationships between variables.\n",
        "Disadvantages:\n",
        "\n",
        "Computationally expensive especially with large datasets.\n",
        "Sensitive to choice of 'k' and distance metric used; inappropriate choices can lead to poor imputations.\n",
        "Machine Learning Models\n",
        "Advanced machine learning models such as Random Forests or Neural Networks can also be employed for imputing missing data by predicting them through learned patterns from complete\n",
        " cases within the dataset.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Capable of capturing complex nonlinear relationships between features.\n",
        "Often provides high accuracy imputations due to sophisticated modeling techniques.\n",
        "Disadvantages:\n",
        "\n",
        "Requires significant computational resources and expertise in machine learning methodologies.\n",
        "Risk overfitting if not properly tuned or validated against independent test sets.\"\"\"\n"
      ],
      "metadata": {
        "id": "L71xNiU9XKy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "\n",
        "\"\"\"Key Factors Affecting Students' Performance in Exams\n",
        "Students' performance in exams is influenced by a myriad of factors that can be broadly categorized into individual, familial, institutional, and environmental domains.\n",
        "Understanding these factors is crucial for educators, policymakers, and researchers aiming to improve educational outcomes. This comprehensive analysis will delve into each\n",
        "category, followed by an exploration of statistical techniques used to analyze these factors.\n",
        "\n",
        "Individual Factors\n",
        "Cognitive Abilities\n",
        "Cognitive abilities such as intelligence quotient (IQ), memory capacity, and problem-solving skills are fundamental determinants of academic performance. Research indicates that\n",
        " students with higher cognitive abilities tend to perform better in exams due to their enhanced ability to process and retain information (The Cambridge Handbook of Intelligence).\n",
        "\n",
        "Motivation and Attitude\n",
        "A student's motivation—both intrinsic and extrinsic—plays a critical role in their academic success. Intrinsic motivation refers to the internal desire to learn for personal\n",
        "satisfaction, while extrinsic motivation involves external rewards such as grades or recognition. Studies have shown that intrinsically motivated students often achieve higher\n",
        "academic success (Educational Psychology: A Cognitive View).\n",
        "\n",
        "Study Habits and Time Management\n",
        "Effective study habits and time management skills are essential for academic achievement. Students who plan their study schedules, set goals, and utilize effective learning\n",
        "strategies tend to perform better on exams (How We Learn: The Surprising Truth About When, Where, and Why It Happens).\n",
        "\n",
        "Familial Factors\n",
        "Socioeconomic Status\n",
        "Socioeconomic status (SES) significantly impacts educational outcomes. Students from higher SES backgrounds typically have access to more educational resources, including books,\n",
        "technology, and tutoring services. This access often translates into better exam performance (Handbook of Research on Student Engagement).\n",
        "\n",
        "Parental Involvement\n",
        "Parental involvement in a child's education is positively correlated with academic success. Parents who engage with their children's schooling by attending meetings, helping with\n",
        "homework, or encouraging educational pursuits contribute positively to their children's exam performance (The Wiley Handbook of Family Psychology).\n",
        "\n",
        "Institutional Factors\n",
        "Quality of Teaching\n",
        "The quality of teaching is a pivotal factor affecting student performance. Teachers who employ effective pedagogical methods, provide constructive feedback, and foster an engaging\n",
        " classroom environment enhance student learning outcomes (Visible Learning for Teachers: Maximizing Impact on Learning).\n",
        "\n",
        "School Environment\n",
        "A supportive school environment that promotes safety, inclusivity, and respect can significantly influence students' academic achievements. Schools that prioritize student\n",
        "well-being tend to see higher levels of student engagement and performance (School Climate: Research Policy Practice).\n",
        "\n",
        "Environmental Factors\n",
        "Peer Influence\n",
        "Peers can have both positive and negative effects on a student's academic performance. Positive peer influence can encourage good study habits and motivate students towards\n",
        "academic excellence (Peer Relationships in Childhood Education).\n",
        "\n",
        "Access to Educational Resources\n",
        "Access to libraries, internet facilities, extracurricular activities, and other educational resources enhances learning opportunities outside the classroom setting. Students with\n",
        " greater access often perform better academically (The Condition of Education).\n",
        "\n",
        "Analyzing Factors Using Statistical Techniques\n",
        "To analyze the factors affecting students' performance in exams statistically:\n",
        "\n",
        "Data Collection\n",
        "Data collection involves gathering quantitative data through surveys or standardized tests measuring various factors like cognitive abilities or socioeconomic status.\n",
        "\n",
        "Descriptive Statistics\n",
        "Descriptive statistics summarize the basic features of the data collected using measures such as mean scores or standard deviations.\n",
        "\n",
        "Inferential Statistics\n",
        "Inferential statistics allow researchers to make predictions or inferences about a population based on sample data:\n",
        "\n",
        "Regression Analysis: Used to determine relationships between dependent variables (exam scores) and independent variables (e.g., SES).\n",
        "ANOVA (Analysis of Variance): Helps compare means across different groups (e.g., comparing exam scores across different teaching methods).\n",
        "Factor Analysis: Identifies underlying relationships between variables by grouping them into factors.\n",
        "Multivariate Analysis\n",
        "Multivariate analysis examines multiple variables simultaneously:\n",
        "\n",
        "Structural Equation Modeling (SEM): Evaluates complex relationships among observed variables.\n",
        "Cluster Analysis: Groups students based on similar characteristics affecting their exam performance.\n",
        "By employing these statistical techniques alongside robust data collection methods from credible sources like standardized assessments or longitudinal studies ensures accurate\n",
        " analysis leading towards actionable insights aimed at improving student outcomes.\"\"\""
      ],
      "metadata": {
        "id": "DtPkL5BcXK1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "\n",
        "\"\"\" Feature Engineering in the Context of Student Performance Data Set\n",
        "Feature engineering is a critical step in the data preprocessing phase of machine learning, particularly when dealing with datasets that aim to predict student performance. This process involves selecting, modifying, and creating features from raw data to improve the predictive power of machine learning models. In the context of a student performance dataset, feature engineering can significantly influence the accuracy and efficiency of predictive models.\n",
        "\n",
        "Understanding the Dataset\n",
        "Before diving into feature engineering, it is essential to understand the nature of the student performance dataset. Typically, such datasets include various attributes related to students' demographics, academic history, socio-economic factors, and possibly psychological or behavioral aspects. Common features might include:\n",
        "\n",
        "Demographic Information: Age, gender, ethnicity.\n",
        "Academic Records: Previous grades, attendance records.\n",
        "Socio-Economic Factors: Parental education levels, family income.\n",
        "Behavioral Aspects: Participation in extracurricular activities, study habits.\n",
        "Understanding these variables helps in identifying which features are likely to be most relevant for predicting student outcomes.\n",
        "\n",
        "Feature Selection\n",
        "Feature selection is about identifying which variables should be included in the model. This step is crucial because irrelevant or redundant features can degrade model performance\n",
        " by introducing noise. Several techniques\n",
        " can be employed for feature selection:\n",
        "\n",
        "Correlation Analysis: Statistical methods such as Pearson correlation coefficients can be used to identify relationships between independent variables and the target variable\n",
        " (student performance). Features with high correlation with the target variable are typically retained.\n",
        "Domain Knowledge: Leveraging insights from educational psychology or pedagogy can guide which features are inherently important based on prior research findings.\n",
        "Recursive Feature Elimination (RFE): This algorithm iteratively removes less significant features based on model weights until an optimal subset is achieved.\n",
        "Principal Component Analysis (PCA): While primarily a dimensionality reduction technique, PCA can help identify which combinations of features capture most variance in data.\n",
        "Feature Transformation\n",
        "Once relevant features are selected, they may need transformation to enhance their utility:\n",
        "\n",
        "Normalization/Standardization: Features like age or test scores may need scaling so that they contribute equally to distance calculations in algorithms like k-nearest neighbors\n",
        "(KNN).\n",
        "Encoding Categorical Variables: Many student datasets include categorical variables such as gender or parental education level. These need conversion into numerical format using\n",
        "techniques like one-hot encoding or label encoding for compatibility with machine learning algorithms.\n",
        "Creation of New Features: Sometimes new informative features can be derived from existing ones through mathematical operations or aggregations—such as calculating average grades\n",
        "over multiple subjects or creating a composite index for socio-economic status.\n",
        "Handling Missing Values: Imputation strategies such as mean/mode substitution or more sophisticated methods like k-nearest neighbors imputation ensure that missing data does not\n",
        "skew results.\n",
        "Binning/Discretization: Continuous variables like age might be binned into categories (e.g., \"teen\", \"young adult\") if it aligns better with how data patterns manifest in real-world\n",
        "scenarios.\n",
        "Model-Specific Considerations\n",
        "Different machine learning models have varying sensitivities to feature types and transformations:\n",
        "\n",
        "Decision trees and ensemble methods like random forests are generally robust to unscaled data but benefit from reduced dimensionality via feature selection.\n",
        "Linear models require careful handling of multicollinearity among predictors.\n",
        "Neural networks often require extensive normalization due to their sensitivity to input scale variations.\n",
        "In conclusion, effective feature engineering requires a blend of statistical analysis techniques and domain-specific insights tailored towards enhancing model interpretability and\n",
        " accuracy within educational contexts.\"\"\""
      ],
      "metadata": {
        "id": "dxBnFNpeXK4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5\n",
        "\n",
        "\"\"\" Exploratory Data Analysis of the Wine Quality Dataset :\n",
        "Introduction to the Wine Quality Dataset:\n",
        "The wine quality dataset is a popular dataset used for classification and regression tasks in machine learning. It consists of various physicochemical properties of wine samples,\n",
        "such as acidity, sugar content, pH levels, and alcohol percentage, along with a quality score assigned by wine experts. The dataset is typically divided into red and white wine\n",
        "samples.\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "Exploratory Data Analysis (EDA) involves summarizing the main characteristics of a dataset often using visual methods. It helps in understanding the patterns, spotting anomalies,\n",
        "testing hypotheses, and checking assumptions through statistical graphics and other data visualization techniques.\n",
        "\n",
        "Distribution Analysis of Features:\n",
        "To perform EDA on the wine quality dataset, we first examine the distribution of each feature to understand their statistical properties:\n",
        "\n",
        "Fixed Acidity: This feature represents the non-volatile acids that do not evaporate easily. The distribution often shows a right-skewed pattern due to higher concentrations in some\n",
        "samples.\n",
        "Volatile Acidity: This measures acetic acid in wine, which at high levels can lead to an unpleasant vinegar taste. Typically exhibits a right-skewed distribution.\n",
        "Citric Acid: Acts as a preservative and adds freshness to wines. Its distribution can be slightly skewed or even normal depending on the type of wine.\n",
        "Residual Sugar: Represents sugars left after fermentation stops naturally or artificially. This feature usually has a right-skewed distribution because most wines have low residual\n",
        "sugar content.\n",
        "Chlorides: Indicates salt content in the wine; generally shows a right-skewed distribution due to low concentration levels in most samples.\n",
        "Free Sulfur Dioxide: Acts as an antimicrobial agent; its distribution is often right-skewed.\n",
        "Total Sulfur Dioxide: Includes both free and bound forms; also tends to be right-skewed.\n",
        "Density: Closely related to alcohol content and sugar level; may show slight skewness but often closer to normality than other features.\n",
        "pH: Measures acidity/alkalinity; typically follows a normal distribution pattern more closely than other features.\n",
        "Sulphates: Contribute to sulfur dioxide levels; usually exhibit right-skewness.\n",
        "Alcohol: Generally follows a normal distribution but can show slight skewness depending on sample diversity.\n",
        "Quality Score: Although ordinal, it is often treated as continuous for analysis purposes; its distribution depends on expert ratings but can be approximately normal if sample sizes are\n",
        "large enough across categories.\n",
        "\n",
        "Identifying Non-Normal Features:\n",
        "Features exhibiting non-normality are those with significant skewness or kurtosis values deviating from zero (for skewness) or three (for kurtosis). In this dataset:\n",
        "\n",
        "Fixed Acidity\n",
        "Volatile Acidity\n",
        "Residual Sugar\n",
        "Chlorides\n",
        "Free Sulfur Dioxide\n",
        "Total Sulfur Dioxide\n",
        "Sulphates\n",
        "These features tend toward non-normal distributions primarily due to their inherent chemical properties and measurement scales used during data collection.\n",
        "\n",
        "Transformations for Improving Normality:\n",
        "To improve normality for these features, several transformations can be applied:\n",
        "\n",
        "Log Transformation: Useful for reducing right skewness by compressing large values more than smaller ones.\n",
        "\n",
        "Square Root Transformation: Effective for moderate skewness reduction while maintaining interpretability.\n",
        "\n",
        "Box-Cox Transformation: A family of power transformations that includes log transformation as a special case; requires positive data.\n",
        "\n",
        "Yeo-Johnson Transformation: Similar to Box-Cox but applicable even when data contains zero or negative values.\n",
        "\n",
        "Reciprocal Transformation: Can handle extreme skewness but may invert order relationships between observations.\n",
        "\n",
        "By applying these transformations appropriately based on initial EDA findings, one can achieve distributions closer to normality which facilitates subsequent statistical analyses\n",
        " like hypothesis testing or linear modeling assumptions validation.\"\"\""
      ],
      "metadata": {
        "id": "jiPN2Lx5Y7XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6\n",
        "\n",
        "\"\"\" Principal Component Analysis (PCA) on Wine Quality Data Set\n",
        "Principal Component Analysis (PCA) is a statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of\n",
        "values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This\n",
        "transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much variability in the data as possible),\n",
        "and each succeeding component in turn has the highest variance possible under the constraint that it be orthogonal to (i.e., uncorrelated with) the preceding components. PCA is\n",
        "sensitive to the relative scaling of the original variables.\n",
        "\n",
        "Understanding PCA:\n",
        "PCA is widely used in exploratory data analysis and for making predictive models. It is often used to reduce dimensionality while preserving as much variance as possible. This\n",
        "reduction can help improve model performance by eliminating noise and redundancy from data, which can lead to overfitting.\n",
        "\n",
        "Steps Involved in PCA:\n",
        "Standardization: Since PCA is affected by scale, it’s important to standardize the data before applying PCA.\n",
        "Covariance Matrix Computation: Calculate the covariance matrix to understand how variables are varying from the mean with respect to each other.\n",
        "Eigenvalues and Eigenvectors: Compute eigenvalues and eigenvectors from the covariance matrix, which will help identify principal components.\n",
        "Feature Vector Formation: Form a feature vector by selecting top k eigenvectors based on their corresponding eigenvalues.\n",
        "Recasting Data Along Principal Components: Transform original dataset along these new axes.\n",
        "\n",
        "Application on Wine Quality Dataset:\n",
        "The wine quality dataset typically consists of several physicochemical properties such as acidity, sugar content, pH level, etc., which are used to predict wine quality scores.\n",
        "\n",
        "Performing PCA:\n",
        "Data Standardization: Each feature should be standardized so that they contribute equally to the distance computations.\n",
        "\n",
        "Covariance Matrix Calculation: Calculate this matrix for all features in the dataset.\n",
        "\n",
        "Eigen Decomposition: Perform an eigen decomposition on this covariance matrix to obtain eigenvalues and eigenvectors.\n",
        "Variance Explained by Each Principal Component:\n",
        "The proportion of variance explained by each component can be calculated using its corresponding eigenvalue divided by the sum of all eigenvalues.\n",
        "Accumulate these proportions until reaching 90% cumulative variance explained.\n",
        "\n",
        "Determining Number of Components:\n",
        "To determine how many principal components are required to explain 90% of variance:\n",
        "\n",
        "Calculate cumulative explained variance ratio for each component.\n",
        "Identify minimum number where cumulative explained variance reaches or exceeds 90%.\n",
        "For example, if after calculating you find that:\n",
        "\n",
        "PC1 explains 40%,\n",
        "PC2 explains 30%,\n",
        "PC3 explains 15%,\n",
        "PC4 explains 5%,\n",
        "Then PCs 1 through 3 cumulatively explain 85%, and adding PC4 would exceed 90%. Thus, four components are necessary."
      ],
      "metadata": {
        "id": "kAmHkWlXY7aI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
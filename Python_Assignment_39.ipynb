{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgeVv58pRIRL"
      },
      "outputs": [],
      "source": [
        "# Q1\n",
        "\"\"\"\n",
        "What is Min-Max Scaling?\n",
        "Min-Max scaling is a normalization technique used in data preprocessing that transforms features to lie within a specified range, typically between 0 and 1. This method is particularly useful when the features have different units or scales, as it ensures that all features contribute equally to the model‚Äôs performance.\n",
        "\n",
        "The formula for Min-Max scaling is given by:\n",
        "X scaled = X ‚àí X min X max ‚àí X min\n",
        "Where: X is the original value, X min is the minimum value of the feature, X max is the maximum value of the feature.\n",
        "\n",
        "How is Min-Max Scaling Used in Data Preprocessing?\n",
        "Min-Max scaling is commonly applied during data preprocessing to prepare datasets for machine learning algorithms. Many algorithms, especially those based on distance calculations (like k-nearest neighbors) or gradient descent (like neural networks), perform better when input features are on a similar scale. By using Min-Max scaling, we can avoid issues where features with larger ranges dominate those with smaller ranges.\n",
        "\n",
        "Example of Min-Max Scaling\n",
        "To illustrate how Min-Max scaling works, consider a simple dataset with two features:\n",
        "\n",
        "Feature_1 \tFeature_2\n",
        "10\t           100\n",
        "20             200\n",
        "30\t           300\n",
        "40\t           400\n",
        "50\t           500\n",
        "\n",
        "Calculate Minimum and Maximum Values:\n",
        "\n",
        "For Feature_1: X min = 10 X max = 50\n",
        "For Feature_2: X min = 100 X max =500\n",
        "\n",
        "Apply Min-Max Scaling: Using the formula for each feature:\n",
        "\n",
        "For Feature_1:\n",
        "\n",
        "Scaled values: 10/50 ‚àí 10/10= 0/40 = 0\n",
        "               20/50 - 10/10 = 10/40 = 0.25\n",
        "               30/50 ‚àí 10/10 = 20/40 = 0.50\n",
        "               40/50 ‚àí 10/10 = 30/40 = 0.75\n",
        "               50/50 ‚àí 10/10 = 40/40 = 1\n",
        "\n",
        "For Feature_2:\n",
        "\n",
        "Scaled values: 100/500 ‚àí 100/100 = 0/400 = 0\n",
        "               200/500 ‚àí 100/100 = 100/400 = 0.25\n",
        "               300/500 ‚àí 100/100 = 200/400 = 0.50\n",
        "               400/500 - 100/100 = 300/400 = 0.75\n",
        "               500/500 ‚àí 100/100 = 400/400 = 1\n",
        "\n",
        "Resulting Scaled Dataset:\n",
        "\n",
        "Feature_1\t    Feature_2\n",
        "0               \t0\n",
        "0.25          \t0.25\n",
        "0.5\t            0.5\n",
        "0.75\t          0.75\n",
        "1\t                1\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\"\"\"\n",
        "Unit Vector Technique in Feature Scaling\n",
        "Feature scaling is a crucial step in data preprocessing, particularly for algorithms that rely on distance calculations, such as k-nearest neighbors and support vector machines.\n",
        "Two common techniques for feature scaling are the Unit Vector technique and Min-Max scaling.\n",
        "\n",
        "Unit Vector Technique\n",
        "The Unit Vector technique, also known as normalization or vector normalization, transforms the features of a dataset into unit vectors. This means that each feature vector is\n",
        "scaled to have a length (or magnitude) of 1. The formula for normalizing a feature vector ùê± is given by:\n",
        "ùêÆ = ùê±‚à•ùê±‚à• where ‚à•ùê±‚à• is the Euclidean norm (or length) of the vector ùê±, calculated as:\n",
        "‚à•ùê±‚à• = under root(‚àëx=n^x = xi^2\n",
        "\n",
        "Steps to Apply the Unit Vector Technique\n",
        "Calculate the Euclidean Norm: For each feature vector, compute its Euclidean norm.\n",
        "Divide Each Component: Divide each component of the feature vector by its norm to obtain the unit vector.\n",
        "Example of Unit Vector Technique\n",
        "Consider a simple dataset with two features:\n",
        "\n",
        "Sample\t      Feature 1\t        Feature 2\n",
        "A\t              3\t                4\n",
        "B\t              1                 2\n",
        "\n",
        "For sample A:\n",
        "\n",
        "Calculate the Euclidean norm: ‚à•ùê±ùêÄ‚à• = Under root (3^2+4^2)= under root (9+16) = underroot 25 = 5\n",
        "Normalize:ùêÆùêÄ = (3/5,4/5)=(0.6,0.8)\n",
        "For sample B:\n",
        "\n",
        "Calculate the Euclidean norm:‚à•ùê±ùêÅ‚à•= under root(1^2+2^2)= under root(1+4)=5\n",
        "Normalize: ùêÆùêÅ = (1/ur(5),2/ur(5)‚âà(0.447,0.894)\n",
        "After applying the Unit Vector technique, our normalized dataset becomes:\n",
        "\n",
        "Sample\tNormalized Feature 1\tNormalized Feature 2\n",
        "A\t          0.6\t                  0.8\n",
        "B\t        0.447\t                  0.894\n",
        "\n",
        "Min-Max Scaling:\n",
        "Min-Max scaling, on the other hand, rescales features to a fixed range, typically [0, 1]. The formula for Min-Max scaling is:\n",
        "x‚Ä≤ = x‚àíxmin / xmax‚àíxmin\n",
        "\n",
        "Differences Between Unit Vector Technique and Min-Max Scaling:\n",
        "Output Range: The Unit Vector technique results in vectors with a magnitude of one but does not constrain values to a specific range like [0,1]. In contrast, Min-Max scaling\n",
        "compresses all values into a specified range.\n",
        "Use Cases: The Unit Vector technique is particularly useful when direction matters more than magnitude (e.g., in text classification using TF-IDF). Min-Max scaling is often used\n",
        " when you want to maintain relationships between features while ensuring they fit within a certain scale.\"\"\"\n"
      ],
      "metadata": {
        "id": "V2lYe1qoUl7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "\n",
        "\"\"\" PCA is rooted in linear algebra and statistics. It involves an orthogonal transformation to convert possibly correlated variables into a set of values of linearly uncorrelated\n",
        " variables called principal components. The number of principal components is less than or equal to the number of original variables.\n",
        "\n",
        "Steps Involved in PCA:\n",
        "Standardization: Since PCA is affected by scale, it is important to standardize the data before applying PCA if the variables have different units or scales.\n",
        "\n",
        "Covariance Matrix Computation: Calculate the covariance matrix to understand how variables are varying from the mean with respect to each other.\n",
        "\n",
        "Eigenvalue Decomposition: Compute eigenvectors and eigenvalues from the covariance matrix. Eigenvectors determine the directions of new feature space, and eigenvalues determine\n",
        "their magnitude or importance.\n",
        "\n",
        "Feature Vector Formation: Select a subset of eigenvectors (principal components) that capture most variance, typically those associated with the largest eigenvalues.\n",
        "\n",
        "Recast Data: Transform original data onto these selected principal components to achieve dimensionality reduction.\n",
        "\n",
        "Application in Dimensionality Reduction\n",
        "Dimensionality reduction through PCA helps simplify models, reduce computational cost, and mitigate overfitting by eliminating noise and redundancy in data while preserving\n",
        "essential patterns.\n",
        "\n",
        "Example Illustration:\n",
        "Consider a dataset containing measurements from various sensors on an industrial machine, with each sensor providing readings on different physical parameters like temperature,\n",
        "pressure, vibration frequency, etc., resulting in high-dimensional data.\n",
        "\n",
        "Data Standardization: Each sensor‚Äôs readings are standardized so they contribute equally to analysis regardless of their unit differences.\n",
        "\n",
        "Covariance Matrix Calculation: A covariance matrix is computed to assess relationships between different sensor readings.\n",
        "\n",
        "Eigen Decomposition: Eigenvalues and eigenvectors are derived from this matrix.\n",
        "\n",
        "Selection of Principal Components: Suppose we find that two principal components capture 95% of total variance; thus, we select these for our reduced feature space.\n",
        "\n",
        "Transformation: Original sensor readings are projected onto these two principal components, reducing dimensions while retaining significant information about machine behavior.\"\"\""
      ],
      "metadata": {
        "id": "76Eyvq2j8cnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "\n",
        "\"\"\" Understanding Principal Component Analysis (PCA):\n",
        "PCA is a method that transforms the original variables of a dataset into a new set of uncorrelated variables called principal components. These components are ordered such that\n",
        " the first few retain most of the variation present in all of the original variables. The primary goal of PCA is to reduce the dimensionality of a dataset while preserving as much\n",
        "  variability as possible.\n",
        "\n",
        "The process involves:\n",
        "\n",
        "Standardizing the Data: This ensures that each variable contributes equally to the analysis.\n",
        "Computing the Covariance Matrix: This matrix captures how variables vary from their mean with respect to each other.\n",
        "Calculating Eigenvectors and Eigenvalues: These are derived from the covariance matrix, where eigenvectors determine the direction of new feature space, and eigenvalues determine\n",
        "their magnitude.\n",
        "Sorting Eigenvectors: They are sorted by decreasing eigenvalues, which indicates their importance.\n",
        "Transforming Data: The original data is transformed into a new subspace using these top eigenvectors.\n",
        "Feature Extraction Using PCA\n",
        "Feature extraction using PCA involves selecting those principal components that capture most of the variance within the data. By doing so, it reduces noise and redundancy,\n",
        "leading to improved efficiency in subsequent analyses or machine learning models.\n",
        "\n",
        "Steps for Feature Extraction with PCA:\n",
        "Data Preprocessing: Normalize or standardize your dataset if necessary.\n",
        "Apply PCA: Compute principal components for your dataset.\n",
        "Select Components: Choose a subset of principal components based on explained variance criteria (e.g., selecting enough components to explain 95% variance).\n",
        "Transform Data: Use these selected components to transform your original dataset into a reduced feature space.\n",
        "Example Illustration\n",
        "Consider an example where we have a dataset containing images represented by pixel values, which can be very high-dimensional (e.g., 1024 dimensions for 32x32 images).\n",
        "Applying PCA can help reduce this dimensionality significantly:\n",
        "Step 1: Standardize pixel values across all images.\n",
        "Step 2: Compute covariance matrix from standardized pixel values.\n",
        "Step 3: Calculate eigenvectors/eigenvalues from this matrix.\n",
        "Step 4: Select top k eigenvectors based on cumulative explained variance (say k=50 for 95% variance).\n",
        "Step 5: Transform image data using these k vectors, resulting in reduced representation while maintaining essential features.\"\"\""
      ],
      "metadata": {
        "id": "RAaEqGm_8czK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Price': [100, 250, 400, 500],\n",
        "        'Rating': [2.5, 3.8, 4.5, 5.0],\n",
        "        'Delivery Time': [20, 35, 50, 60]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbdWkj1-8c16",
        "outputId": "abb48898-0ecf-44bf-d73a-93420b7e0b0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Price  Rating  Delivery Time\n",
            "0  0.000    0.00          0.000\n",
            "1  0.375    0.52          0.375\n",
            "2  0.750    0.80          0.750\n",
            "3  1.000    1.00          1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Revenue': [100, 200, 300, 400, 500],\n",
        "        'Profit Margin': [10, 15, 20, 25, 30],\n",
        "        'P/E Ratio': [12, 14, 18, 20, 25],\n",
        "        'Trading Volume': [1000, 1500, 2000, 2500, 3000]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "print(\"Explained Variance:\", pca.explained_variance_ratio_)\n",
        "\n",
        "print(pd.DataFrame(df_pca, columns=['PC1', 'PC2']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Jre5sx8c4t",
        "outputId": "81136a38-2b60-4086-91d1-ec9fdcf96a78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance: [0.99568751 0.00431249]\n",
            "        PC1       PC2\n",
            "0 -2.755068  0.120674\n",
            "1 -1.475397 -0.110156\n",
            "2  0.021747  0.037887\n",
            "3  1.301418 -0.192942\n",
            "4  2.907300  0.144538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "x_min, x_max = data.min(), data.max()\n",
        "\n",
        "a, b = -1, 1\n",
        "\n",
        "scaled_data = ((data - x_min) / (x_max - x_min)) * (b - a) + a\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWaWMNvi8c7k",
        "outputId": "057e27d2-b74f-4614-92fa-cdb3a753221d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1], [5], [10], [15], [20]])\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9LAbJb68c-H",
        "outputId": "9020f841-43a4-4fbb-d0e5-1aa710b6e353"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = {\n",
        "    'Height': [170, 165, 180, 175, 160],\n",
        "    'Weight': [65, 70, 80, 75, 60],\n",
        "    'Age': [25, 30, 35, 40, 22],\n",
        "    'Gender': [0, 1, 0, 1, 0],  # Male=0, Female=1\n",
        "    'Blood Pressure': [120, 130, 140, 125, 115]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(df_scaled)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", explained_variance)\n",
        "print(\"Cumulative Variance:\", cumulative_variance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm5LIiqS8dBf",
        "outputId": "24a83f52-1738-464d-899b-5c89e4892039"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [7.06366512e-01 2.22089768e-01 6.53253201e-02 6.21839936e-03\n",
            " 3.77241818e-35]\n",
            "Cumulative Variance: [0.70636651 0.92845628 0.9937816  1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FDKC7_Xy_PHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}